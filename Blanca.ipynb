{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A frequency ranked list of economics vocabulary\n",
    "\n",
    "### Aim of project:\n",
    "To help a friend improve her economics specific English vocabulary in an efficient way. \n",
    "\n",
    "### End result of project:\n",
    "1. A <a href=\"https://github.com/pvonglehn/economics_vocab/blob/master/economics_vocab.txt\">list</a> of the most common, economics specific English words which don't have Spanish cognates. That is, words which are common in economics texts but uncommon in general texts and which cannot be easily guessed by a Spanish speaker. \n",
    "<br><br>\n",
    "2. Anki flashcards <a href=\"https://github.com/pvonglehn/economics_vocab/blob/master/Blanca.apkg\">deck</a> for studying these words with example sentences.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem:\n",
    "A friend is preparing for an English exam as part of her studies to become an economist for the civil service in Spain. Part of the exam will be based on an article from <a href=\"https://www.economist.com/\">'the economist'</a>, the International Monetary Fund <a href=\"https://www.imf.org/external/pubs/ft/fandd/\">finance and development magazine</a>, or a similar publication. She finds that she is lacking much of the economics specific vocabulary necessary to understand these articles. \n",
    "\n",
    "The classic strategy for improving vocabulary is to read a lot and look up words that you don't know. However, this approach is very inefficient. \n",
    "\n",
    "Let's consider an example.\n",
    "The student reads the sentence:<br>\n",
    "'Higher <strong>wages</strong> in China make <strong>offshoring</strong> less attractive.'\n",
    "\n",
    "The student may not know the meaning of 'wages' or 'offshoring' so their instinct might be to look up and learn both. What the student doesn't know however, is that while the word 'wage' is very common in financial texts and definitely worth learning,  the word 'offshoring' is much less common, so it is not worth the student's effort learning, at least as long as there are many more useful words they could learn first.\n",
    "\n",
    "The student should learn the most relevant words first, which is what this project aims to help with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project results:\n",
    "\n",
    "Below is a table showing the first 10 words in the final list. The economics_rank indicates each word's rank based on its frequency of occurrence in the imf and 'the economist' magazines. This corpus was compiled during this project and consists of over four million tokens (words) from over 4000 articles. The general rank indicates the word's position on a list of 5000 English words ordered by their frequency of occurence in a large corpus constructed from a wide range of English texts. This list was downloaded from https://www.wordfrequency.info/ \n",
    "\n",
    "This list only contains words which are more common in the economics corpus than in general English, so 'stop words' like 'the','and' etc. don't appear. We can see that the top words in our list occur much more frequently in the economics corpus than in general English. This indicates that these words are very important to know in order to understand these texts, but students may not know them because they are relatively uncommon in general texts that they will mostly have been exposed to in their English studies. \n",
    "\n",
    "<br><em>A general_rank of 1000000 means that the word is not on the list from the general corpus.</em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economics_rank</th>\n",
       "      <th>economics_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>inbox</th>\n",
       "      <td>174</td>\n",
       "      <td>2781</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upgrade</th>\n",
       "      <td>214</td>\n",
       "      <td>2410</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>240</td>\n",
       "      <td>2163</td>\n",
       "      <td>1684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investor</th>\n",
       "      <td>311</td>\n",
       "      <td>1761</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asset</th>\n",
       "      <td>315</td>\n",
       "      <td>1714</td>\n",
       "      <td>1869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spending</th>\n",
       "      <td>329</td>\n",
       "      <td>1640</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currency</th>\n",
       "      <td>348</td>\n",
       "      <td>1487</td>\n",
       "      <td>3297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decline</th>\n",
       "      <td>349</td>\n",
       "      <td>1470</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revenue</th>\n",
       "      <td>421</td>\n",
       "      <td>1212</td>\n",
       "      <td>1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wage</th>\n",
       "      <td>450</td>\n",
       "      <td>1131</td>\n",
       "      <td>2300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          economics_rank  economics_freq  general_rank\n",
       "inbox                174            2781       1000000\n",
       "upgrade              214            2410       1000000\n",
       "debt                 240            2163          1684\n",
       "investor             311            1761          1536\n",
       "asset                315            1714          1869\n",
       "spending             329            1640          2082\n",
       "currency             348            1487          3297\n",
       "decline              349            1470          1790\n",
       "revenue              421            1212          1514\n",
       "wage                 450            1131          2300"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economics_vocab_no_cognates.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words with Spanish cognates\n",
    "\n",
    "Below is the table including words with Spanish cognates. The meanings of 'percent', 'minister', 'sector' etc could all be easily guessed by a Spanish speaker. Of the top 1000 words on this list, 40% of the words have Spanish cognates, so it is really worth excluding them from our list to save the student's time and effort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economics_rank</th>\n",
       "      <th>economics_freq</th>\n",
       "      <th>general_rank</th>\n",
       "      <th>spanish_cognate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>percent</th>\n",
       "      <td>157</td>\n",
       "      <td>2976</td>\n",
       "      <td>1000000</td>\n",
       "      <td>por ciento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inbox</th>\n",
       "      <td>174</td>\n",
       "      <td>2781</td>\n",
       "      <td>1000000</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upgrade</th>\n",
       "      <td>214</td>\n",
       "      <td>2410</td>\n",
       "      <td>1000000</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minister</th>\n",
       "      <td>224</td>\n",
       "      <td>2294</td>\n",
       "      <td>1711</td>\n",
       "      <td>ministro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>240</td>\n",
       "      <td>2163</td>\n",
       "      <td>1684</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>253</td>\n",
       "      <td>2052</td>\n",
       "      <td>1000000</td>\n",
       "      <td>acuerdo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sector</th>\n",
       "      <td>298</td>\n",
       "      <td>1824</td>\n",
       "      <td>1767</td>\n",
       "      <td>sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investor</th>\n",
       "      <td>311</td>\n",
       "      <td>1761</td>\n",
       "      <td>1536</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asset</th>\n",
       "      <td>315</td>\n",
       "      <td>1714</td>\n",
       "      <td>1869</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spending</th>\n",
       "      <td>329</td>\n",
       "      <td>1640</td>\n",
       "      <td>2082</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          economics_rank  economics_freq  general_rank spanish_cognate\n",
       "percent              157            2976       1000000      por ciento\n",
       "inbox                174            2781       1000000               -\n",
       "upgrade              214            2410       1000000               -\n",
       "minister             224            2294          1711        ministro\n",
       "debt                 240            2163          1684               -\n",
       "accord               253            2052       1000000         acuerdo\n",
       "sector               298            1824          1767          sector\n",
       "investor             311            1761          1536               -\n",
       "asset                315            1714          1869               -\n",
       "spending             329            1640          2082               -"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_cognate.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anki Flashcards\n",
    "\n",
    "Anki is a flashcard app that uses active recall testing and spaced repetition to help you learn almost anything very efficiently. The flash cards in this project contain three components: 1) A sentence in English with the word to be learned/tested highlighted in blue. 2) The translation of this word and of the full sentence in Spanish. 3) A dictionary entry with translations into Spanish.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"./images/Anki1.png\">\n",
    "<br>\n",
    "Above you can see the front of the flashcard, which asks the user to give the meaning of the word in blue. The user then has to guess the correct preposition (say it aloud or in your head - you don't type anything into Anki). And then press \"show answer\" to see if you are correct. \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"./images/Anki2.png\">\n",
    "<br>\n",
    "When the \"show answer\" button is pressed, the Spanish translation or the single word and of the full phrase is given. If the user wants to they can click \"show definition\" to get a full dictionary definition of the card.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"./images/Anki3.png\">\n",
    "<br>\n",
    "The full dictionary definition is given. The dictionary information was scraped from the site http://www.spanishdict.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Procedural overview\n",
    "\n",
    "1. Download the imf articles in Spanish and English\n",
    "2. Split the text into sentences and align Spanish and English sentence pairs\n",
    "3. Add sentences from the economist magazine to sentences\n",
    "4. Tokenize the sentences (split into words)\n",
    "5. Lemmatize the words (put into dictionary form e.g. running -> run)\n",
    "6. Rank all words by frequency\n",
    "7. Get a separate frequency ranked list of English words from a general corpus of English texts\n",
    "8. Get a list of Spanish-English cognates (e.g. la economía <-> the economy)\n",
    "9. Create our list of most common economics specific words without Spanish cognates \n",
    "10. Get dictionary entries with examples for each word by web scraping\n",
    "11. Extract examples from dictionary entries\n",
    "12. Make the Anki flash cards\n",
    "\n",
    "\n",
    "## Tools used\n",
    "Beautiful Soup for web scraping<br>\n",
    "NLTK for natural language processing<br>\n",
    "pandas for data manipulation<br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do/ improvements to be made/ features to add\n",
    "\n",
    "Include examples from the sentences aligned parallel corpus in the Anki cards\n",
    "\n",
    "\n",
    "Resolve issues caused by question marks in regular expressions when making the Anki deck\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the text corpus\n",
    "### Get a sentence aligned parallel corpus of imf magazine articles\n",
    "Fortunately, the imf published much of its material in mutliple languages. Having downloaded all of the articles from the beginning of 2009 to the end of 2017 in English and Spanish I then needed match up each English sentence with each Spanish sentence (for creating flash cards with examples later). The first step was to split up the articles into sentences using nltk package in python. I then used a external package called hunalign to match up each English sentences with its Spanish counterpart.\n",
    "\n",
    "\n",
    "hunalign:<br>\n",
    "https://github.com/danielvarga/hunalign<br>\n",
    "D. Varga, L. Németh, P. Halácsy, A. Kornai, V. Trón, V. Nagy (2005).<br>\n",
    "Parallel corpora for medium density languages<br>\n",
    "In Proceedings of the RANLP 2005<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the imf articles in Spanish and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pdfs of the imf articles from 2009 to 2017 from the imf website\n",
    "\n",
    "cmd = \"mkdir pdfs\"\n",
    "os.system(cmd)\n",
    "for year in range(2009,2018):\n",
    "    for month in [\"03\",\"06\",\"09\",\"12\"]:\n",
    "        short_year = str(year)[-2:]\n",
    "        cmd = \"\"\"curl \"https://www.imf.org/external/pubs/ft/fandd/{}/{}/pdf/fd{}{}.pdf\" \\\n",
    "                --output ./pdfs/eng-{}-{}.pdf\"\"\".format(year,month,month,short_year,year,month)\n",
    "        os.system(cmd)\n",
    "        cmd = \"\"\"curl \"https://www.imf.org/external/pubs/ft/fandd/spa/{}/{}/pdf/fd{}{}s.pdf\" \\\n",
    "                --output ./pdfs/spa-{}-{}.pdf\"\"\".format(year,month,month,short_year,year,month)\n",
    "        os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the text into sentences and align Spanish and English sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 2.15 s, total: 2min 30s\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "# Create the sentences aligned parallel corpus\n",
    "\n",
    "%%time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "sentence_df = pd.DataFrame({\"english\":[],\"spanish\":[]})\n",
    "cmd = \"rm -rf tmp ; mkdir tmp\"\n",
    "os.system(cmd)\n",
    "for year in range(2009,2018):\n",
    "    for month in [\"03\",\"06\",\"09\",\"12\"]:\n",
    "        \n",
    "        # convert the pdfs to raw text\n",
    "        cmd = \"\"\"xpdf-tools-mac-4.00/bin64/pdftotext \\\n",
    "        ./pdfs/eng-{}-{}.pdf  tmp/eng-{}-{}.txt\"\"\".format(year,month,year,month)\n",
    "        os.system(cmd)\n",
    "        cmd = \"\"\"xpdf-tools-mac-4.00/bin64/pdftotext \\\n",
    "        ./pdfs/spa-{}-{}.pdf  tmp/spa-{}-{}.txt\"\"\".format(year,month,year,month)\n",
    "        os.system(cmd)\n",
    "        e = open(\"tmp/eng-{}-{}.txt\".format(year,month),\"r\",encoding=\"Latin-1\").read()\n",
    "        s = open(\"tmp/spa-{}-{}.txt\".format(year,month),\"r\",encoding=\"Latin-1\").read()\n",
    "        \n",
    "        # split the texts into sentences\n",
    "        english_sentences = nltk.sent_tokenize(e)\n",
    "        spanish_sentences = nltk.sent_tokenize(s)\n",
    "        eo = open(\"tmp/sent-eng-{}-{}.txt\".format(year,month),\"w\")\n",
    "        for line in english_sentences:\n",
    "            eo.write((line+\"\\n\"))\n",
    "        eo.close()\n",
    "        so = open(\"tmp/sent-spa-{}-{}.txt\".format(year,month),\"w\")\n",
    "        for line in spanish_sentences:\n",
    "            so.write((line+\"\\n\"))\n",
    "        so.close()\n",
    "        \n",
    "        #Align the sentences with external package hunalign\n",
    "        cmd = \"\"\"hunalign/hunalign-1.2/src/hunalign/hunalign \\\n",
    "        hunalign/hunalign-1.2/data/null.dic \\\n",
    "        tmp/sent-eng-{}-{}.txt tmp/sent-spa-{}-{}.txt\\\n",
    "        -text -bisent > tmp/aligned-{}-{}.txt\"\"\".format(year,month,year,month,year,month)\n",
    "        os.system(cmd)\n",
    "\n",
    "        #put lines into dataframe\n",
    "        tmp_df = pd.DataFrame({\"english\":[],\"spanish\":[]})\n",
    "        lines = open(\"tmp/aligned-{}-{}.txt\".format(year,month),\"r\").readlines()\n",
    "        for line in lines:\n",
    "            sentences = line.split(\"\\t\")\n",
    "            eng = sentences[0]\n",
    "            spa = sentences[1]\n",
    "            tmp_df = tmp_df.append(pd.DataFrame({\"english\":[eng],\"spanish\":[spa]}))\n",
    "        sentence_df = sentence_df.append(tmp_df)\n",
    "        \n",
    "sentence_df.to_csv(\"imf_sentences.txt\",sep=\"\\t\",index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imf_sentences = pd.read_csv(\"imf_sentences.txt\",sep=\"\\t\",header=None)\n",
    "all_imf_sentences.columns = \"english\",\"spanish\"\n",
    "all_imf_sentences = all_imf_sentences.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short sentences to avoid titles and other non complete sentences\n",
    "\n",
    "long_imf_sentences = all_imf_sentences[all_imf_sentences['english'].apply(lambda x: len(x.split()) > 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence count = 46668\n",
      "total word count = 1005216\n",
      "mean words per sentence = 21.54\n"
     ]
    }
   ],
   "source": [
    "sentence_count = len(long_imf_sentences['english'])\n",
    "word_count = long_imf_sentences['english'].apply(lambda x : len(x.split())).sum()\n",
    "words_per_sentence = long_imf_sentences['english'].apply(lambda x : len(x.split())).mean()\n",
    "print(\"sentence count = {}\\ntotal word count = {}\\nmean words per sentence = {:.2f}\"\n",
    "      .format(sentence_count,word_count,words_per_sentence))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Add sentences from the economist magazine to corpus\n",
    "\n",
    "Adding words from a year's worth of articles from the economist magazine.  The economist sentences were provided already in csv format by a friend. I am not including the raw economist data in this repository because the articles require a subscription to acces and I do not have permission to distribute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_economist = pd.read_csv(\"the_economist.csv\",encoding=\"Latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_economist['words'] = the_economist['words'].str.replace(\"[',?]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_break(text):\n",
    "    text = re.sub(\"\\[|\\]\",\"\",text)\n",
    "    for match in re.findall(\"\\D\\.\\s\",text):\n",
    "        try:\n",
    "            regex = match[0] + \"\\.\"\n",
    "            text = re.sub(regex,(regex[0] + \"\\t\"),text)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 28.8 s, total: 52.6 s\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tab_separated = the_economist['words'].apply(sentence_break).sum()\n",
    "split_sentences = tab_separated.split(\"\\t\")\n",
    "split_sentences = pd.Series(split_sentences)\n",
    "long_econ_sentences = split_sentences[split_sentences.apply(lambda x: len(x.split()) > 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the economist and imf articles\n",
    "imf_econ = pd.concat([long_imf_sentences['english'],long_econ_sentences],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14               SENIOR EDITORS Camilla Andersen Archana Kumar James Rowe Simon Willson\n",
       "15      ASSISTANT EDITORS Maureen Burke Sergio Negrete Cardenas Natalie Ramirez-Djumena\n",
       "18                 EDITORIAL ASSISTANTS Lijun Li Kelley McCollum Niccole Braynen-Kimani\n",
       "20    Periodicals postage is paid at Washington, DC, and at additional mailing offices.\n",
       "21            The English edition is printed at United Lithographers Inc., Ashburn, VA.\n",
       "dtype: object"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imf_econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article count = 4440\n",
      "sentence count = 213807\n",
      "total word count = 4523948\n",
      "mean words per sentence = 21.16\n"
     ]
    }
   ],
   "source": [
    "sentence_count = len(imf_econ)\n",
    "word_count = imf_econ.apply(lambda x : len(x.split())).sum()\n",
    "words_per_sentence = imf_econ.apply(lambda x : len(x.split())).mean()\n",
    "articles = len(the_economist)\n",
    "print(\"article count = {}\\nsentence count = {}\\ntotal word count = {}\\nmean words per sentence = {:.2f}\"\n",
    "      .format(articles,sentence_count,word_count,words_per_sentence))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenize the sentences (split into words)\n",
    "### 5. Lemmatize the words (put into dictionary form e.g. running -> run)\n",
    "\n",
    "We need to turn our tokens (words) into their lemmas i.e. their dictionary forms https://en.wikipedia.org/wiki/Lemma_(morphology)\n",
    "<br>\n",
    "E.g. running will be turned into run and played into play\n",
    "To do this we need to:\n",
    "\n",
    "1. tokenize the sentences (break them up into words)\n",
    "2. tag the parts of speech for each token (word) e.g. verb, adjective\n",
    "3. lemmatize the tokens (turn into dictionary form)\n",
    "\n",
    "Full sentences need to be passed to the parts of speech tagger in order to tag them accurately\n",
    "If the word \"play\" is given in isolation, it is ambiguous if it is a verb or a noun,\n",
    "but if you give the pos tagger a full sentence, it can determine from context\n",
    "e.g. I am going to play (verb) tennis. I am going to see a play (noun).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/pv7409/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pv7409/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# I am using the natural language toolkit python library\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# This function converts the parts of speech tags from nltk pos tagger \n",
    "# To POS tags that are compatible with the wordnet lemmatizer\n",
    "# function adapted from: \n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 43s, sys: 5.84 s, total: 5min 49s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Lemmatizing all the tokens\n",
    "\n",
    "all_tokens = []\n",
    "all_lemmas = []\n",
    "for sentence in imf_econ:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokenized = nltk.pos_tag(tokens)\n",
    "    for i, token in enumerate(tokenized):\n",
    "        # Filter out proper nouns (words with capital letters that aren't at the beginning of a sentence)\n",
    "        if token[0].islower() or i == 0:\n",
    "            word = token[0].lower()\n",
    "            wordnet_pos = get_wordnet_pos(token[1])\n",
    "        #print(word,wordnet_pos)\n",
    "            if wordnet_pos is None:\n",
    "                all_lemmas.append(word)\n",
    "                all_tokens.append(word)\n",
    "            else:\n",
    "                all_lemmas.append(lemmatizer.lemmatize(word,wordnet_pos))\n",
    "                all_tokens.append(word)\n",
    "\n",
    "# write out lemmes to text file\n",
    "f = open(\"all_lemmas.txt\",\"w\")\n",
    "for i in all_lemmas:\n",
    "    f.write((i+\"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll process the lemmas a little\n",
    "\n",
    "all_lemmas = open(\"all_lemmas.txt\",\"r\").read().split(\"\\n\")\n",
    "\n",
    "all_lemma_series = pd.Series(all_lemmas)\n",
    "\n",
    "#remove punctuation\n",
    "all_lemma_series = all_lemma_series[~all_lemma_series.str.contains(\"\\W\")]\n",
    "\n",
    "# remove proper names\n",
    "propernames = pd.Series(open(\"/usr/share/dict/propernames\",\"r\").read().split(\"\\n\"))\n",
    "propernames = propernames.apply(lambda x: x.lower())\n",
    "propernames = set(propernames)\n",
    "#all_lemma_series = all_lemma_series[~all_lemma_series.isin(propernames)]\n",
    "\n",
    "#get rid of numbers\n",
    "all_lemma_series = all_lemma_series[~all_lemma_series.str.contains(\"\\d\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Rank all words by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency table for each lemma\n",
    "# Give each lemma a rank\n",
    "\n",
    "lemma_frequencey = all_lemma_series.value_counts()\n",
    "ranked = pd.Series(lemma_frequencey.index)\n",
    "ranks = pd.DataFrame(list(range(1,len(lemma_frequencey)+1)))\n",
    "ranks.index = lemma_frequencey.index\n",
    "ranks['freq'] = lemma_frequencey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economics_rank</th>\n",
       "      <th>economics_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>8</td>\n",
       "      <td>55367</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>9</td>\n",
       "      <td>54685</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>15</td>\n",
       "      <td>26638</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>16</td>\n",
       "      <td>22019</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>21</td>\n",
       "      <td>17803</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      economics_rank  economics_freq  general_rank\n",
       "have               8           55367             8\n",
       "that               9           54685            11\n",
       "with              15           26638            15\n",
       "from              16           22019            25\n",
       "more              21           17803            78"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that I had previously removed 1,2 and 3 letter words\n",
    "\n",
    "ranks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get a frequency ranked list of English words from a general corpus of English texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of the 5000 most frequent words in English from https://www.wordfrequency.info/\n",
    "<br>Note that although there are 5000 entries in the list, there are only 4353 unique words,\n",
    "as sometimes the same word has several entries because it appears as a different part of speech \n",
    "<br>e.g. 'light' appears as a noun (the light at the end of the tunnel) and as an adjective (a light breakfast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of English words by frequency, generated from 14 billion word intente corpus\n",
    "# https://www.wordfrequency.info/ \n",
    "f = open(\"5000_eng_words.txt\",\"r\")\n",
    "freq_5000 = f.read().split(\"\\n\")\n",
    "freq_5000 = freq_5000[1:] # ignore header\n",
    "freq_5000_list = pd.Series(freq_5000).str.lower()\n",
    "\n",
    "#remove duplicated words in freq_5000_list\n",
    "freq_5000_list = pd.Series(freq_5000_list).unique()\n",
    "\n",
    "# give each word in the general corpus frequency list a rank\n",
    "freq5000df = pd.DataFrame(list(range(1,len(freq_5000_list)+1)))\n",
    "freq5000df.index = freq_5000_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "the  1\n",
       "be   2\n",
       "and  3\n",
       "of   4\n",
       "a    5"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq5000df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the general corpus frequency data frame with our economics vocab frequency list\n",
    "\n",
    "ranks = ranks.merge(freq5000df,how=\"left\",left_index=True,right_index=True)\n",
    "ranks.columns = \"economics_rank\",\"economics_freq\",\"general_rank\"\n",
    "ranks = ranks.sort_values(\"economics_rank\")\n",
    "\n",
    "# if word not in general corpus list, give it rank of 1000000\n",
    "ranks.loc[ranks[\"general_rank\"].isnull(),[\"general_rank\"]] = 1000000 \n",
    "ranks[\"general_rank\"] = ranks[\"general_rank\"].astype(int) # turn back into integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove small words (get rid of single letters etc.)\n",
    "words_series = pd.Series(ranks.index)\n",
    "long_words = words_series[words_series.apply(lambda x: len(x) > 3)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ranks.loc[long_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economics_rank</th>\n",
       "      <th>economics_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>8</td>\n",
       "      <td>55367</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>9</td>\n",
       "      <td>54685</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>15</td>\n",
       "      <td>26638</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>16</td>\n",
       "      <td>22019</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>21</td>\n",
       "      <td>17803</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      economics_rank  economics_freq  general_rank\n",
       "have               8           55367             8\n",
       "that               9           54685            11\n",
       "with              15           26638            15\n",
       "from              16           22019            25\n",
       "more              21           17803            78"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Get a list of Spanish-English cognates (e.g. la economía <-> the economy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of spanish/english cognates downloaded from: http://cognates.org/pdf/mfcogn.pdf\n",
    "# Here I'm cleaning up the text file a little (formatting was messed up when converted from pdf)\n",
    "\n",
    "f = open(\"cognates.txt\",\"r\").read()\n",
    "f = re.sub(r'\\((.*?)\\)',\",\",f)\n",
    "f = re.sub(\"por ciento\",\"porciento\",f)\n",
    "f = re.sub(\"se relajó\",\"serelajó\",f)\n",
    "f = re.sub(\"en el presente\",\"enelpresente\",f)\n",
    "f = re.sub(\"ex prefix\",\"prefix\",f)\n",
    "f = re.sub(\"ex prefijo\",\"prefix\",f)\n",
    "f = re.sub(\"soul, música\",\"soulmúsica\",f)\n",
    "f = re.sub(\"substituir v. 5/sustituir\",\"substituir/sustituir\",f)\n",
    "f = re.sub(\"rock n' roll\",\"rock'n'roll\",f)\n",
    "f = re.sub(\"El Salvador\",\"ElSalvador\",f)\n",
    "f = re.sub(\"valuación, avalúo\",\"valuación/avalúo\",f)\n",
    "f = re.sub(\"prefix\",\"\",f)\n",
    "f = re.sub(\"intj.\",\"\",f)\n",
    "\n",
    "f = re.sub(r'\\d',\",\",f)\n",
    "f = re.split(\"PMF|MFW| |conj\\.|v\\.|adj\\.|n\\.|s\\.|adv\\.|,|abbr\\.|abr\\.|prep\\.\",f)\n",
    "to_remove = \"Cognate\",\"org\",\"\",\"clic\",\"prefijo\",\"prefix\"\n",
    "for item in to_remove:\n",
    "    while item in f: f.remove(item)\n",
    "f[f.index(\"porciento\")] = \"por ciento\"\n",
    "f[f.index(\"serelajó\")] = \"se relajó\"\n",
    "f[f.index(\"enelpresente\")] = \"en el presente\"\n",
    "f[f.index(\"soulmúsica\")] = \"soul música\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognate_list = []\n",
    "for i in range(0,len(f)-1,2):\n",
    "    cognate_list.append([f[i],f[i+1]])\n",
    "cognate_df = pd.DataFrame(cognate_list)\n",
    "cognate_df = pd.DataFrame(cognate_df)\n",
    "cognate_df.columns = \"english\",\"spanish\"\n",
    "cognate_df.to_csv(\"spanish_english_cognates.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing list of spanish-english cognates to file\n",
    "\n",
    "cognate_df = pd.read_csv(\"spanish_english_cognates.csv\",header=None)\n",
    "cognate_df.columns = \"english\",\"spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>english</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a.m.</td>\n",
       "      <td>a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>abandonó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>abandonado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     english     spanish\n",
       "0    english     spanish\n",
       "1       a.m.        a.m.\n",
       "2    abandon   abandonar\n",
       "3  abandoned    abandonó\n",
       "4  abandoned  abandonado"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cognate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put english word on index of data frame\n",
    "cog_df = pd.DataFrame(cognate_df[\"spanish\"])\n",
    "cog_df.index = cognate_df[\"english\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Create our list of most common economics specific words without Spanish cognates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all words which have a rank of 1000 ranks higher in the economist corpus than in the general corpus\n",
    "economics_vocab = ranks[((ranks[\"economics_rank\"] ) < ranks[\"general_rank\"] - 1000) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove words which have spanish cognates\n",
    "\n",
    "s = pd.Series(economics_vocab.index)\n",
    "no_cognates = list(s[~s.isin(cognate_df[\"english\"])])\n",
    "have_cognates = list(s[s.isin(cognate_df[\"english\"])])\n",
    "\n",
    "#  dataframe without words which have spanish cognates\n",
    "economics_vocab_no_cognates = economics_vocab.loc[no_cognates]\n",
    "\n",
    "# datafram with only words which have spanish cognates\n",
    "economics_vocab_with_cognates = economics_vocab.loc[have_cognates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame with all words plus the spanish cognates for those words which have them\n",
    "w_cognate = economics_vocab.merge(cog_df,how=\"left\",left_index=True,right_index=True)\n",
    "w_cognate = w_cognate.sort_values(\"economics_rank\")\n",
    "w_cognate.loc[w_cognate['spanish'].isnull(),['spanish']] = \"-\"\n",
    "w_cognate = w_cognate.rename({\"spanish\":\"spanish_cognate\"},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economics_rank</th>\n",
       "      <th>economics_freq</th>\n",
       "      <th>general_rank</th>\n",
       "      <th>spanish_cognate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>percent</th>\n",
       "      <td>157</td>\n",
       "      <td>2976</td>\n",
       "      <td>1000000</td>\n",
       "      <td>por ciento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inbox</th>\n",
       "      <td>174</td>\n",
       "      <td>2781</td>\n",
       "      <td>1000000</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upgrade</th>\n",
       "      <td>214</td>\n",
       "      <td>2410</td>\n",
       "      <td>1000000</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minister</th>\n",
       "      <td>224</td>\n",
       "      <td>2294</td>\n",
       "      <td>1711</td>\n",
       "      <td>ministro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>240</td>\n",
       "      <td>2163</td>\n",
       "      <td>1684</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          economics_rank  economics_freq  general_rank spanish_cognate\n",
       "percent              157            2976       1000000      por ciento\n",
       "inbox                174            2781       1000000               -\n",
       "upgrade              214            2410       1000000               -\n",
       "minister             224            2294          1711        ministro\n",
       "debt                 240            2163          1684               -"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_cognate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vocab list\n",
    "f = open(\"economics_vocab.txt\",\"w\")\n",
    "f.write(\"\"\"#List of 2000 economics related words \n",
    "#Ranked by frequency of occurrence in the imf finance and development magazine\n",
    "#https://www.imf.org/external/pubs/ft/fandd/\n",
    "#Words with Spanish cognates removed\\n\"\"\")\n",
    "for i in economics_vocab_no_cognates[:2000].index:\n",
    "    f.write((i+\"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Get dictionary entries with examples for each word by web scraping\n",
    "\n",
    "Dictionary entries scraped from http://www.spanishdict.com/\n",
    "<br>These include several direct translations and example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "economics_vocab_list = open(\"economics_vocab.txt\",\"r\").readlines()\n",
    "economics_vocab_list = pd.Series(economics_vocab_list)\n",
    "economics_vocab_list = economics_vocab_list.iloc[4:]\n",
    "economics_vocab_list = economics_vocab_list.str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 4.24 s, total: 1min 50s\n",
      "Wall time: 19min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "definitions = pd.DataFrame({\"word\":[],\"definitions\":[]})\n",
    "for word in economics_vocab_list:\n",
    "    try:\n",
    "        url=\"http://www.spanishdict.com/translate/{}\".format(word)\n",
    "        content = requests.get(url).content\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "        entry = str(soup.select(\".dictionary-entry\")[0])\n",
    "        tmp_df = pd.DataFrame({\"word\":[word],\"definitions\":[entry]})\n",
    "        definitions = definitions.append(tmp_df)\n",
    "    except:\n",
    "        tmp_df = pd.DataFrame({\"word\":[word],\"definitions\":[np.nan]})\n",
    "        definitions = definitions.append(tmp_df)\n",
    "definitions.to_csv(\"defintions_4_anki.txt\",header=None,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = pd.read_csv(\"defintions_4_anki.txt\",header=None,sep=\"\\t\")\n",
    "definitions.columns=\"word\",\"definitions\"\n",
    "definitions.index = definitions['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of words on the first draught of the list that my friend says she already knows\n",
    "\n",
    "knows = pd.read_csv(\"words_blanca_knows.txt\",header=None)[1:]\n",
    "knows.loc[220:,1] = \"x\"\n",
    "already_knows = set(knows[knows[1].isnull()][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Extract examples from dictionary entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in definitions['word'][:400]:\n",
    "    try:\n",
    "        content = definitions.loc[word,'definitions']\n",
    "        soup = BeautifulSoup(content,'lxml')\n",
    "        for i, item in enumerate(soup.find_all(class_=\"dictionary-neodict-example\")):\n",
    "            trans = item.parent.previousSibling.find_all(class_=\"dictionary-neodict-translation-translation\")[0].text\n",
    "            if trans == \"\":\n",
    "                trans == \"No direct translation\"\n",
    "            english = item.find_all(\"span\")[0].text\n",
    "            spanish = item.find_all(class_=\"exB\")[0].text\n",
    "            definitions.loc[word,'ex{}trans'.format(i)] = trans \n",
    "            definitions.loc[word,'ex{}eng'.format(i)] = english\n",
    "            definitions.loc[word,'ex{}spa'.format(i)] = spanish\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Make the Anki flash cards:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with I will only make cards for around the first 200 words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check = definitions[~definitions['word'].isin(already_knows)]\n",
    "to_check = to_check.iloc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_check[:200].to_csv(\"to_check.csv\",sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_anki = pd.read_csv(\"./first_195.csv\",sep=\"\\t\",index_col=0,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### make anki for guessing English word from Spanish\n",
    "\n",
    "def make_anki(entry,replacement,sentence,translation,extra,definition):\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokenized = nltk.pos_tag(tokens)\n",
    "        lemmas = []\n",
    "        for i, token in enumerate(tokenized):\n",
    "            word = token[0]\n",
    "            wordnet_pos = get_wordnet_pos(token[1])\n",
    "            if wordnet_pos is None:\n",
    "                lemmas.append(word)\n",
    "            else:\n",
    "                lemmas.append(lemmatizer.lemmatize(word,wordnet_pos))\n",
    "        target_index = lemmas.index(entry)\n",
    "        tokens[target_index] = \"{{c1::\" + replacement + \"::\" + tokens[target_index] + \"}}\"\n",
    "        tokens[-2] = tokens[-2] + tokens[-1]\n",
    "        result = \" \".join(tokens[:-1])\n",
    "        return (result + \"\\t\" + extra + \"\\t\" + translation + \"\\t\" + definition)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# entry = \"scheme\"\n",
    "# replacement = \"el plan\"\n",
    "# sentence = \"The city government developed a scheme to revitalize its downtown area.\"\n",
    "# translation = \"El gobierno municipal ideó un plan para revitalizar el centro de la ciudad.\"\n",
    "# extra = \" \"\n",
    "# definition = \"definition goes here\"\n",
    "# make_anki(word,replacement,sentence,translation,extra,definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"¿Qué significa la palabra en azul?<br><br>The city government developed a <span style='color:blue;'> scheme</span> to revitalize its downtown area.\\t<strong>el plan</strong><br>El gobierno municipal ideó un plan para revitalizar el centro de la ciudad.\\tdefinition goes here\""
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Make anki for guessing Spanish word from English\n",
    "\n",
    "def make_anki(entry,replacement,sentence,translation,extra,definition):\n",
    "    \"\"\"makes an Anki card with sentence in English, translation in Spanish and definition\"\"\"\n",
    "    try:\n",
    "        # Split the sentence into tokens (words plus puncuation)\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokenized = nltk.pos_tag(tokens)\n",
    "        lemmas = []\n",
    "        # Lemmatize the words (necessary for replacing/highlighting the words later)\n",
    "        for i, token in enumerate(tokenized):\n",
    "            word = token[0]\n",
    "            wordnet_pos = get_wordnet_pos(token[1])\n",
    "            if wordnet_pos is None:\n",
    "                lemmas.append(word)\n",
    "            else:\n",
    "                lemmas.append(lemmatizer.lemmatize(word,wordnet_pos))\n",
    "        target_index = lemmas.index(entry)\n",
    "        # insert css styling around the target word\n",
    "        tokens[target_index] = \"<span style='color:blue;'> \" + tokens[target_index] + \"</span>\"\n",
    "        tokens[-2] = tokens[-2] + tokens[-1]\n",
    "        result = \" \".join(tokens[:-1])\n",
    "        result = re.sub(\" n't\",\"n't\",result)\n",
    "        for match in re.findall(\"\\w\\w\\w \\W\",result):\n",
    "            result = re.sub(match,re.sub(\" \",\"\",r\"{}\".format(match)),result)\n",
    "             \n",
    "\n",
    "        return (\"¿Qué significa la palabra en azul?<br><br>\" + result + \"\\t\" + \"<strong>\" + replacement + \"</strong><br>\" + translation + \"\\t\" + definition)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Lines below are a test of the make_anki function    \n",
    "entry = \"scheme\"\n",
    "replacement = \"el plan\"\n",
    "sentence = \"The city government developed a scheme to revitalize its downtown area.\"\n",
    "translation = \"El gobierno municipal ideó un plan para revitalizar el centro de la ciudad.\"\n",
    "extra = \" \"\n",
    "definition = \"definition goes here\"\n",
    "make_anki(word,replacement,sentence,translation,extra,definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary where the keys are the words to be learned and the \n",
    "# values are a list of anki cards with one example per card\n",
    "\n",
    "cards = {}\n",
    "for entry in for_anki.index:\n",
    "    try:\n",
    "#        cards[entry] = [\"{{c1::what is meaning of...::what is meaning of...}}<br>\" + entry + \"\\t\" + \" \" + \"\\t\" + definitions.loc[entry,\"definitions\"] + \"\\t\" + \" \"  ]\n",
    "        cards[entry] = []\n",
    "        for i in range(for_anki.loc[entry].dropna().shape[0]//3):\n",
    "            replacement = for_anki.loc[entry,1+(i*3)]\n",
    "            sentence = for_anki.loc[entry,2+(i*3)]\n",
    "            translation = for_anki.loc[entry,3+(i*3)]\n",
    "            definition = definitions.loc[entry,'definitions']\n",
    "            extra = \" \"\n",
    "            result = make_anki(entry,replacement,sentence,translation,extra,definition)\n",
    "            if result is None:\n",
    "                continue\n",
    "            else:\n",
    "                cards[entry].append(result)\n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the deck for loading into Anki\n",
    "# We want the easier cards to appear first, but we also want to randomize\n",
    "# the order a little to spread out examples of the same words to make\n",
    "# the deck less predicatable\n",
    "\n",
    "import copy\n",
    "import random\n",
    "f = open(\"test_anki_deck.txt\",\"w\")\n",
    "# deep copy required because we are removing cards from the list and don't want to\n",
    "# alter the original list\n",
    "destroy_cards = copy.deepcopy(cards) \n",
    "# Group the cards in intervals of 15 and randomize within those groups\n",
    "interval = 15\n",
    "written_cards = []\n",
    "for start in range(0,len(for_anki),interval):\n",
    "    randlist = []\n",
    "    # Take 3 examples for each word, or the most available if less than 3\n",
    "    for i in range(4):\n",
    "        for entry in for_anki.index[start:start+interval]:\n",
    "            if len(destroy_cards[entry]) > 0:\n",
    "                randlist.append(destroy_cards[entry].pop(0))\n",
    "    while len(randlist) > 0:\n",
    "        next_card = randlist.pop(random.randrange(len(randlist)))\n",
    "        f.write(next_card)\n",
    "        f.write(\"\\n\")\n",
    "        written_cards.append(next_card.split(\"\\t\")[0])           \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Anki deck can then be imported in as a tab seperated file. I needed to create a special card time in Anki to accomodate the front/back/definition format of the card. These cards types are already in the .apgk file in this repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
