{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A frequency ranked list of economics vocabulary\n",
    "\n",
    "### Aim of project:\n",
    "To help a friend improve her economics specific English vocabulary in an efficient way. \n",
    "\n",
    "### End result of project:\n",
    "1. A <a href=\"https://github.com/pvonglehn/economics_vocab/blob/master/economics_vocab.txt\">list</a> of the most common, economics specific English words which don't have Spanish cognates. That is, words which are common in economics texts but uncommon in general texts and which cannot be easily guessed by a Spanish speaker. \n",
    "<br><br>\n",
    "2. Anki flashcards deck for studying these words with example sentences (still to do)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem:\n",
    "A friend is preparing for an English exam as part of her studies to become an economist for the civil service in Spain. Part of the exam will be based on an article from International Monetary Fund <a href=\"https://www.imf.org/external/pubs/ft/fandd/\">finance and development magazine</a>, or a similar publication. She finds that she is lacking much of the economics specific vocabulary necessary to understand these articles. \n",
    "\n",
    "The classic strategy for improving vocabulary is to read a lot and look up words that you don't know. However, this approach is very inefficient. \n",
    "\n",
    "Let's consider an example.\n",
    "The student reads the sentence:<br>\n",
    "'Higher <strong>wages</strong> in China make <strong>offshoring</strong> less attractive.'\n",
    "\n",
    "The student may not know the meaning of 'wages' or 'offshoring' so their instinct might be to look up and learn both. What the student doesn't know however, is that while the word 'wage' is very common in financial texts and definitely worth learning,  the word 'offshoring' is much less common, so it is not worth the student's effort learning, at least as long as there are many more useful words they could learn first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project results:\n",
    "\n",
    "Below is a table showing the first 5 words in the final list. The imf_rank indicates each word's rank based on its frequency of occurrence in the imf magazine. This corpus was compiled during this project and consists of over two million tokens (words) from over 100,000 sentences from over 1000 articles. The general rank indicates the word's position on a list of 5000 English words ordered by their frequency of occurence in a large corpus of a wide range of English texts. This list was downloaded from https://www.wordfrequency.info/ \n",
    "\n",
    "<br><em>A general_rank of 1000000 means that the word is not on the list from the general corpus.</em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imf_rank</th>\n",
       "      <th>imf_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>currency</th>\n",
       "      <td>136</td>\n",
       "      <td>2328</td>\n",
       "      <td>3297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poverty</th>\n",
       "      <td>146</td>\n",
       "      <td>2123</td>\n",
       "      <td>2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spending</th>\n",
       "      <td>175</td>\n",
       "      <td>1853</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inequality</th>\n",
       "      <td>206</td>\n",
       "      <td>1556</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>228</td>\n",
       "      <td>1432</td>\n",
       "      <td>3476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            imf_rank  imf_freq  general_rank\n",
       "currency         136      2328          3297\n",
       "poverty          146      2123          2080\n",
       "spending         175      1853          2082\n",
       "inequality       206      1556       1000000\n",
       "banking          228      1432          3476"
      ]
     },
     "execution_count": 1157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economics_vocab_no_cognates.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the table including words with Spanish cognates. The meanings of 'percent', 'fiscal' and 'export' could all be easily guessed by a Spanish speaker, so we are taking them off the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imf_rank</th>\n",
       "      <th>imf_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>percent</th>\n",
       "      <td>32</td>\n",
       "      <td>6845</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fiscal</th>\n",
       "      <td>98</td>\n",
       "      <td>2952</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currency</th>\n",
       "      <td>136</td>\n",
       "      <td>2328</td>\n",
       "      <td>3297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poverty</th>\n",
       "      <td>146</td>\n",
       "      <td>2123</td>\n",
       "      <td>2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>export</th>\n",
       "      <td>157</td>\n",
       "      <td>2036</td>\n",
       "      <td>3062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          imf_rank  imf_freq  general_rank\n",
       "percent         32      6845       1000000\n",
       "fiscal          98      2952          3212\n",
       "currency       136      2328          3297\n",
       "poverty        146      2123          2080\n",
       "export         157      2036          3062"
      ]
     },
     "execution_count": 1158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economics_vocab.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Procedural overview\n",
    "1. Generate a text corpus by web scraping economics articles \n",
    "2. Split the text into sentences and then tag each word by its part of speech (verb, noun etc.)\n",
    "3. Convert the words into their lemmas (dictionary entry forms). e.g. running -> run\n",
    "4. Create a frequency table of the words and put them in rank order\n",
    "5. Get an existing ranked list of words from a corpus of general English\n",
    "6. Produce a list of words which have a higher rank in the economics corpus than in the general corpus\n",
    "7. Remove words which have Spanish cognates e.g. the economy - la econom√≠a\n",
    "8. Get example sentences with Spanish translation for each word\n",
    "9. Turn these example sentences into flash cards for studying\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do/ improvements to be made/ features to add\n",
    "\n",
    "Get example sentences and make Anki flashcards.<br>\n",
    "Get more articles e.g. from 'the economist'.\n",
    "<br>Use an existing web crawling library such as https://scrapy.org/ to crawl entire sites in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web crawling function to get all links from a webpage using beautiful soup\n",
    "\n",
    "# WARNING: Take care before reusing this function. It is one of my first attempts at \n",
    "# writing a function to scrape multiple pages and is quite hacky. It works for this case but may not\n",
    "# work as desired in other cases.\n",
    "\n",
    "# Arguments are:\n",
    "# url: starting url\n",
    "# base_url: the url of website homepage\n",
    "# must_include: regex that the links must include (default is the wildcard character \".\")\n",
    "# must_not_include: regex that the links must not include (default is the nonsense string \"xasdfcasdf\" )\n",
    "\n",
    "def get_all_links(url,base_url,existing_links,must_include = \".\",must_not_include = \"xasdfcasdf\"):\n",
    "    content = requests.get(url).content\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "    links = []\n",
    "    existing_links = existing_links.copy() # this copy is required to prevent modifying the original list (side effect)\n",
    "    for anchor in soup.findAll(\"a\"):\n",
    "        # if the anchor element isn't doesn't have a href, it isn't a proper url, so skip to next anchor\n",
    "        try:\n",
    "            link = anchor['href']\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "        full_link = None \n",
    "        if re.search(\"^http\",link): # if link starts with http it is either an external link or internal with full url\n",
    "            if base_url in link: # exclude links to external sites\n",
    "                full_link = link \n",
    "        elif re.search(\"^/\",link): # if link is an internat link to the base_url, create full link from base_url\n",
    "            full_link = (base_url + \"/\" + anchor['href'])\n",
    "        else:\n",
    "            full_link = (url + \"/\" + anchor['href']) # if link is\n",
    "            \n",
    "        # filter out links based on various conditions    \n",
    "        if ((full_link not in existing_links) # ignore links that have already been found\n",
    "             and full_link is not None\n",
    "             and (not re.search(\"#\",full_link)) # filter out links to id's on the same page\n",
    "             and (not re.search(\"htm.*htm\",full_link)) # this is hack because of some buggy behaviour - should fix\n",
    "             and  re.search(\"htm$\",full_link) \n",
    "             and re.search(must_include,full_link)\n",
    "             and (not re.search(must_not_include,full_link))):\n",
    "            \n",
    "            # add to list of links found on the whole site so far\n",
    "            # (the existing_links variable here has local scope only)\n",
    "            existing_links.append(full_link)\n",
    "            \n",
    "            # add to list of links found within this function invokation\n",
    "            links.append(full_link)\n",
    "            \n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 3.35 s, total: 1min 6s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get all internal links from the imf finance and development publications website\n",
    "\n",
    "f = open(\"imf_links.txt\",\"w\") # file to save the links\n",
    "\n",
    "base_url = \"https://www.imf.org\"\n",
    "must_not_include = \"fandd/spa|fandd/fre|fandd/rus|fandd/chi|fandd/ara|fandd/ger\"\n",
    "all_links = []\n",
    "\n",
    "# The online magazine is published quaterly\n",
    "# loop over the years and quaters\n",
    "for year in range(1996,2019):\n",
    "    for month in (\"03\",\"06\",\"09\",\"12\"):\n",
    "        must_include = \"external/pubs/ft/fandd/{}/{}\".format(year,month) # only get links from current edition\n",
    "        current_existing = [] # initialise list of links that have been visited within this loop\n",
    "        current_existing.append(\"https://www.imf.org/external/pubs/ft/fandd/{}/{}\".format(year,month))\n",
    "        for link in current_existing:\n",
    "            # get all new links from within this link and add them to the list of links already found\n",
    "            current_existing.extend(get_all_links(link,base_url,current_existing,must_include,must_not_include))\n",
    "        all_links.extend(current_existing)\n",
    "        for link in current_existing:\n",
    "            f.write((link + \"\\n\"))\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 3.1 s, total: 1min 12s\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Visit each link\n",
    "# Extract all text\n",
    "# Break the text into sentences with nltk (natural language tool kit) sentence tokenizer \n",
    "\n",
    "import nltk\n",
    "all_sentences = []\n",
    "for link in all_links:\n",
    "    content = requests.get(link).content\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "    text = soup.body.text\n",
    "    sentences = nltk.sent_tokenize(text) # this splits the text into sentences\n",
    "    all_sentences.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude sentences which are shorter than 6 words\n",
    "# As they are probably not proper sentences\n",
    "\n",
    "long_sentences = []\n",
    "for sentence in all_sentences:\n",
    "    if sentence.count(\"\\n\") < 2:\n",
    "        if len(sentence.split()) > 6:\n",
    "            long_sentences.append(sentence)\n",
    "\n",
    "f = open(\"imf_sentences.txt\",\"w\")\n",
    "for line in long_sentences:\n",
    "    f.write((line + \"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence count = 101541\n",
      "total word count = 2470329\n",
      "mean words per sentence = 24.33\n"
     ]
    }
   ],
   "source": [
    "sentence_count = len(long_sentences)\n",
    "word_count = pd.Series(long_sentences).apply(lambda x : len(x.split())).sum()\n",
    "words_per_sentence = pd.Series(long_sentences).apply(lambda x : len(x.split())).mean()\n",
    "print(\"sentence count = {}\\ntotal word count = {}\\nmean words per sentence = {:.2f}\"\n",
    "      .format(sentence_count,word_count,words_per_sentence))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/pv7409/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pv7409/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# There are libraries and functions that we will need to turn the\n",
    "# words into their lemmas (dictionary forms) https://en.wikipedia.org/wiki/Lemma_(morphology)\n",
    "# E.g. running will be turned into run and played into play\n",
    "# To do this we need to:\n",
    "\n",
    "# 1. tokenize the sentences (break them up into words)\n",
    "# 2. tag the parts of speech for each token (word) e.g. verb, adjective\n",
    "# 3. lemmatize the tokens (turn into dictionary form)\n",
    "\n",
    "# part of speech to the lemmatizer\n",
    "# Full sentences need to be passed to the parts of speech tagger in order to tag them accurately\n",
    "# If the word \"play\" is given in isolation, it is ambiguous if it is a verb or a noun,\n",
    "# but if you give the pos tagger a full sentence, it can determine from context\n",
    "# e.g. I am going to play (verb) tennis. I am going to see a play (noun).\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# This function converts the parts of speech tags from nltk pos tagger \n",
    "# To POS tags that are compatible with the wordnet lemmatizer\n",
    "# function adapted from: \n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 25s, sys: 4.73 s, total: 3min 30s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_tokens = []\n",
    "all_lemmas = []\n",
    "for sentence in long_sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokenized = nltk.pos_tag(tokens)\n",
    "    for i, token in enumerate(tokenized):\n",
    "        # Filter out proper nouns (words with capital letters that aren't at the beginning of a sentence)\n",
    "        if token[0].islower() or i == 0:\n",
    "            word = token[0].lower()\n",
    "            wordnet_pos = get_wordnet_pos(token[1])\n",
    "        #print(word,wordnet_pos)\n",
    "            if wordnet_pos is None:\n",
    "                all_lemmas.append(word)\n",
    "                all_tokens.append(word)\n",
    "            else:\n",
    "                all_lemmas.append(lemmatizer.lemmatize(word,wordnet_pos))\n",
    "                all_tokens.append(word)\n",
    "\n",
    "# write out lemmes to text file\n",
    "f = open(\"all_lemmas.txt\",\"w\")\n",
    "for i in all_lemmas:\n",
    "    f.write((i+\"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No we'll process the lemmas a little\n",
    "\n",
    "all_lemma_series = pd.Series(all_lemmas)\n",
    "\n",
    "#remove punctuation\n",
    "all_lemma_series = all_lemma_series[~all_lemma_series.str.contains(\"\\W\")]\n",
    "\n",
    "# remove proper names\n",
    "propernames = pd.Series(open(\"/usr/share/dict/propernames\",\"r\").read().split(\"\\n\"))\n",
    "propernames = propernames.apply(lambda x: x.lower())\n",
    "propernames = set(propernames)\n",
    "#all_lemma_series = all_lemma_series[~all_lemma_series.isin(propernames)]\n",
    "\n",
    "#get rid of numbers\n",
    "all_lemma_series = all_lemma_series[~all_lemma_series.str.contains(\"\\d\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency table for each lemma\n",
    "# Give each lemma a rank\n",
    "\n",
    "lemma_frequencey = all_lemma_series.value_counts()\n",
    "ranked = pd.Series(lemma_frequencey.index)\n",
    "ranks = pd.DataFrame(list(range(1,len(lemma_frequencey)+1)))\n",
    "ranks.index = lemma_frequencey.index\n",
    "ranks['freq'] = lemma_frequencey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>149311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2</td>\n",
       "      <td>85426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3</td>\n",
       "      <td>82856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>4</td>\n",
       "      <td>74219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>5</td>\n",
       "      <td>69129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    freq\n",
       "the  1  149311\n",
       "of   2   85426\n",
       "and  3   82856\n",
       "be   4   74219\n",
       "to   5   69129"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of the 5000 most frequent words in English from https://www.wordfrequency.info/\n",
    "<br>Note that although there are 5000 entries in the list, there are only 4353 unique words,\n",
    "as sometimes the same word has several entries because it appears as a different part of speech \n",
    "<br>e.g. 'light' appears as a noun (the light at the end of the tunnel) and as an adjective (a light breakfast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of English words by frequency, generated from 14 billion word intente corpus\n",
    "# https://www.wordfrequency.info/ \n",
    "f = open(\"5000_eng_words.txt\",\"r\")\n",
    "freq_5000 = f.read().split(\"\\n\")\n",
    "freq_5000 = freq_5000[1:] # ignore header\n",
    "freq_5000_list = pd.Series(freq_5000).str.lower()\n",
    "\n",
    "#remove duplicated words in freq_5000_list\n",
    "freq_5000_list = pd.Series(freq_5000_list).unique()\n",
    "\n",
    "# give each word in the general corpus frequency list a rank\n",
    "freq5000df = pd.DataFrame(list(range(1,len(freq_5000_list)+1)))\n",
    "freq5000df.index = freq_5000_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "the  1\n",
       "be   2\n",
       "and  3\n",
       "of   4\n",
       "a    5"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq5000df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the general corpus frequency data frame with our economics vocab frequency list\n",
    "\n",
    "ranks = ranks.merge(freq5000df,how=\"left\",left_index=True,right_index=True)\n",
    "ranks.columns = \"imf_rank\",\"imf_freq\",\"general_rank\"\n",
    "ranks = ranks.sort_values(\"imf_rank\")\n",
    "\n",
    "# if word not in general corpus list, give it rank of 1000000\n",
    "ranks.loc[ranks[\"general_rank\"].isnull(),[\"general_rank\"]] = 1000000 \n",
    "ranks[\"general_rank\"] = ranks[\"general_rank\"].astype(int) # turn back into integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imf_rank</th>\n",
       "      <th>imf_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>149311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2</td>\n",
       "      <td>85426</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3</td>\n",
       "      <td>82856</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>4</td>\n",
       "      <td>74219</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>5</td>\n",
       "      <td>69129</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     imf_rank  imf_freq  general_rank\n",
       "the         1    149311             1\n",
       "of          2     85426             4\n",
       "and         3     82856             3\n",
       "be          4     74219             2\n",
       "to          5     69129             7"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude short words\n",
    "large_words = pd.Series(ranks.index)\n",
    "large_words = large_words[large_words.apply(lambda x : len(x) > 3)]\n",
    "ranks = ranks.loc[list(large_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Work evaporated at the port on the south shore of the giant estuary, the Rio de la Plata.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_series = pd.Series(long_sentences)\n",
    "my_series[my_series.str.contains(\"estuary\")].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of spanish/english cognates downloaded from: http://cognates.org/pdf/mfcogn.pdf\n",
    "# massage the text file (formatting was messed up when converted from pdf)\n",
    "f = open(\"cognates.txt\",\"r\").read()\n",
    "f = re.sub(r'\\((.*?)\\)',\",\",f)\n",
    "f = re.sub(\"por ciento\",\"porciento\",f)\n",
    "f = re.sub(\"se relaj√≥\",\"serelaj√≥\",f)\n",
    "f = re.sub(\"en el presente\",\"enelpresente\",f)\n",
    "f = re.sub(\"ex prefix\",\"prefix\",f)\n",
    "f = re.sub(\"ex prefijo\",\"prefix\",f)\n",
    "f = re.sub(\"soul, m√∫sica\",\"soulm√∫sica\",f)\n",
    "f = re.sub(\"substituir v. 5/sustituir\",\"substituir/sustituir\",f)\n",
    "f = re.sub(\"rock n' roll\",\"rock'n'roll\",f)\n",
    "f = re.sub(\"El Salvador\",\"ElSalvador\",f)\n",
    "f = re.sub(\"valuaci√≥n, aval√∫o\",\"valuaci√≥n/aval√∫o\",f)\n",
    "f = re.sub(\"prefix\",\"\",f)\n",
    "f = re.sub(\"intj.\",\"\",f)\n",
    "\n",
    "f = re.sub(r'\\d',\",\",f)\n",
    "f = re.split(\"PMF|MFW| |conj\\.|v\\.|adj\\.|n\\.|s\\.|adv\\.|,|abbr\\.|abr\\.|prep\\.\",f)\n",
    "to_remove = \"Cognate\",\"org\",\"\",\"clic\",\"prefijo\",\"prefix\"\n",
    "for item in to_remove:\n",
    "    while item in f: f.remove(item)\n",
    "f[f.index(\"porciento\")] = \"por ciento\"\n",
    "f[f.index(\"serelaj√≥\")] = \"se relaj√≥\"\n",
    "f[f.index(\"enelpresente\")] = \"en el presente\"\n",
    "f[f.index(\"soulm√∫sica\")] = \"soul m√∫sica\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognate_list = []\n",
    "for i in range(0,len(f)-1,2):\n",
    "    cognate_list.append([f[i],f[i+1]])\n",
    "cognate_df = pd.DataFrame(cognate_list)\n",
    "cognate_df = pd.DataFrame(cognate_df)\n",
    "cognate_df.columns = \"english\",\"spanish\"\n",
    "cognate_df.to_csv(\"spanish_english_cognates.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>servant</td>\n",
       "      <td>sirviente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>completed</td>\n",
       "      <td>completado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>attribute</td>\n",
       "      <td>atributo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>communist</td>\n",
       "      <td>comunista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>attitude</td>\n",
       "      <td>actitud</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        english     spanish\n",
       "3373    servant   sirviente\n",
       "687   completed  completado\n",
       "315   attribute    atributo\n",
       "658   communist   comunista\n",
       "308    attitude     actitud"
      ]
     },
     "execution_count": 1096,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cognate_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#economics_vocab = ranks[(ranks[\"imf_rank\"] > ranks[\"general_rank\"] + 500) & (ranks[\"general_rank\"] < 10000)]\n",
    "economics_vocab = ranks[((ranks[\"imf_rank\"] ) < ranks[\"general_rank\"]) & (ranks[\"general_rank\"] > 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(economics_vocab.index)\n",
    "no_cognates = list(s[~s.isin(cognate_df[\"english\"])])\n",
    "have_cognates = list(s[s.isin(cognate_df[\"english\"])])\n",
    "economics_vocab_no_cognates = economics_vocab.loc[no_cognates]\n",
    "economics_vocab_with_cognates = economics_vocab.loc[have_cognates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imf_rank</th>\n",
       "      <th>imf_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>percent</th>\n",
       "      <td>32</td>\n",
       "      <td>6845</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fiscal</th>\n",
       "      <td>98</td>\n",
       "      <td>2952</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>export</th>\n",
       "      <td>157</td>\n",
       "      <td>2036</td>\n",
       "      <td>3062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monetary</th>\n",
       "      <td>170</td>\n",
       "      <td>1892</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inflation</th>\n",
       "      <td>181</td>\n",
       "      <td>1823</td>\n",
       "      <td>2830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>207</td>\n",
       "      <td>1551</td>\n",
       "      <td>2492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advanced</th>\n",
       "      <td>220</td>\n",
       "      <td>1473</td>\n",
       "      <td>2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>external</th>\n",
       "      <td>240</td>\n",
       "      <td>1362</td>\n",
       "      <td>2536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shock</th>\n",
       "      <td>264</td>\n",
       "      <td>1240</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economist</th>\n",
       "      <td>285</td>\n",
       "      <td>1150</td>\n",
       "      <td>2650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stability</th>\n",
       "      <td>302</td>\n",
       "      <td>1077</td>\n",
       "      <td>2624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>productivity</th>\n",
       "      <td>309</td>\n",
       "      <td>1041</td>\n",
       "      <td>3650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reduction</th>\n",
       "      <td>330</td>\n",
       "      <td>1003</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure</th>\n",
       "      <td>340</td>\n",
       "      <td>986</td>\n",
       "      <td>3192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>import</th>\n",
       "      <td>349</td>\n",
       "      <td>969</td>\n",
       "      <td>3555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reserve</th>\n",
       "      <td>351</td>\n",
       "      <td>966</td>\n",
       "      <td>3299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio</th>\n",
       "      <td>353</td>\n",
       "      <td>964</td>\n",
       "      <td>2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consumption</th>\n",
       "      <td>360</td>\n",
       "      <td>955</td>\n",
       "      <td>2788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incentive</th>\n",
       "      <td>366</td>\n",
       "      <td>939</td>\n",
       "      <td>2421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dollar</th>\n",
       "      <td>379</td>\n",
       "      <td>903</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transfer</th>\n",
       "      <td>383</td>\n",
       "      <td>896</td>\n",
       "      <td>2066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integration</th>\n",
       "      <td>384</td>\n",
       "      <td>896</td>\n",
       "      <td>3164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>structural</th>\n",
       "      <td>397</td>\n",
       "      <td>857</td>\n",
       "      <td>3379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative</th>\n",
       "      <td>427</td>\n",
       "      <td>802</td>\n",
       "      <td>2166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>429</td>\n",
       "      <td>793</td>\n",
       "      <td>2819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transaction</th>\n",
       "      <td>430</td>\n",
       "      <td>792</td>\n",
       "      <td>3366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pension</th>\n",
       "      <td>444</td>\n",
       "      <td>771</td>\n",
       "      <td>3336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjustment</th>\n",
       "      <td>447</td>\n",
       "      <td>763</td>\n",
       "      <td>2764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rapid</th>\n",
       "      <td>465</td>\n",
       "      <td>740</td>\n",
       "      <td>2682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implement</th>\n",
       "      <td>489</td>\n",
       "      <td>697</td>\n",
       "      <td>2037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>institutional</th>\n",
       "      <td>496</td>\n",
       "      <td>691</td>\n",
       "      <td>2837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recession</th>\n",
       "      <td>507</td>\n",
       "      <td>679</td>\n",
       "      <td>3166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>529</td>\n",
       "      <td>643</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distribution</th>\n",
       "      <td>566</td>\n",
       "      <td>583</td>\n",
       "      <td>2115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficiency</th>\n",
       "      <td>574</td>\n",
       "      <td>570</td>\n",
       "      <td>2699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsidy</th>\n",
       "      <td>577</td>\n",
       "      <td>567</td>\n",
       "      <td>3738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>substantial</th>\n",
       "      <td>582</td>\n",
       "      <td>565</td>\n",
       "      <td>2267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recovery</th>\n",
       "      <td>585</td>\n",
       "      <td>561</td>\n",
       "      <td>2181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deposit</th>\n",
       "      <td>601</td>\n",
       "      <td>548</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovation</th>\n",
       "      <td>606</td>\n",
       "      <td>546</td>\n",
       "      <td>3230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objective</th>\n",
       "      <td>611</td>\n",
       "      <td>539</td>\n",
       "      <td>2144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corruption</th>\n",
       "      <td>630</td>\n",
       "      <td>524</td>\n",
       "      <td>3302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mechanism</th>\n",
       "      <td>633</td>\n",
       "      <td>520</td>\n",
       "      <td>2379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rapidly</th>\n",
       "      <td>634</td>\n",
       "      <td>520</td>\n",
       "      <td>2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implication</th>\n",
       "      <td>653</td>\n",
       "      <td>492</td>\n",
       "      <td>2171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prospect</th>\n",
       "      <td>701</td>\n",
       "      <td>454</td>\n",
       "      <td>2071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficient</th>\n",
       "      <td>703</td>\n",
       "      <td>454</td>\n",
       "      <td>2654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stable</th>\n",
       "      <td>709</td>\n",
       "      <td>446</td>\n",
       "      <td>2569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sustain</th>\n",
       "      <td>717</td>\n",
       "      <td>442</td>\n",
       "      <td>2643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expansion</th>\n",
       "      <td>721</td>\n",
       "      <td>437</td>\n",
       "      <td>2427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>destined</th>\n",
       "      <td>17228</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noble</th>\n",
       "      <td>17241</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recuperate</th>\n",
       "      <td>17316</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obligated</th>\n",
       "      <td>17359</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>users</th>\n",
       "      <td>17455</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auxiliary</th>\n",
       "      <td>17475</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stressed</th>\n",
       "      <td>17635</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consults</th>\n",
       "      <td>17677</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gratitude</th>\n",
       "      <td>17774</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encountered</th>\n",
       "      <td>17811</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nations</th>\n",
       "      <td>17919</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solitary</th>\n",
       "      <td>17933</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torture</th>\n",
       "      <td>17940</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>packet</th>\n",
       "      <td>17978</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hydrogen</th>\n",
       "      <td>17987</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupied</th>\n",
       "      <td>18008</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obtained</th>\n",
       "      <td>18040</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communities</th>\n",
       "      <td>18084</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deserted</th>\n",
       "      <td>18086</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>islands</th>\n",
       "      <td>18089</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directs</th>\n",
       "      <td>18106</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interrupted</th>\n",
       "      <td>18108</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favour</th>\n",
       "      <td>18122</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted</th>\n",
       "      <td>18160</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exceptions</th>\n",
       "      <td>18178</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salvation</th>\n",
       "      <td>18216</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>academy</th>\n",
       "      <td>18219</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comprehension</th>\n",
       "      <td>18238</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ignorant</th>\n",
       "      <td>18280</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquired</th>\n",
       "      <td>18282</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mama</th>\n",
       "      <td>18298</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homosexual</th>\n",
       "      <td>18305</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violently</th>\n",
       "      <td>18342</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ignored</th>\n",
       "      <td>18385</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proposes</th>\n",
       "      <td>18394</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innocence</th>\n",
       "      <td>18464</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assuming</th>\n",
       "      <td>18532</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obstacles</th>\n",
       "      <td>18612</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posture</th>\n",
       "      <td>18642</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>princess</th>\n",
       "      <td>18696</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divinity</th>\n",
       "      <td>18708</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demon</th>\n",
       "      <td>18767</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caused</th>\n",
       "      <td>18783</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qualities</th>\n",
       "      <td>18830</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salute</th>\n",
       "      <td>18875</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restitution</th>\n",
       "      <td>18892</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenery</th>\n",
       "      <td>18912</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invited</th>\n",
       "      <td>19074</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salaries</th>\n",
       "      <td>19114</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studied</th>\n",
       "      <td>19116</td>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1182 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                imf_rank  imf_freq  general_rank\n",
       "percent               32      6845       1000000\n",
       "fiscal                98      2952          3212\n",
       "export               157      2036          3062\n",
       "monetary             170      1892       1000000\n",
       "inflation            181      1823          2830\n",
       "finance              207      1551          2492\n",
       "advanced             220      1473          2483\n",
       "external             240      1362          2536\n",
       "shock                264      1240          2050\n",
       "economist            285      1150          2650\n",
       "stability            302      1077          2624\n",
       "productivity         309      1041          3650\n",
       "reduction            330      1003          2024\n",
       "infrastructure       340       986          3192\n",
       "import               349       969          3555\n",
       "reserve              351       966          3299\n",
       "ratio                353       964          2735\n",
       "consumption          360       955          2788\n",
       "incentive            366       939          2421\n",
       "dollar               379       903       1000000\n",
       "transfer             383       896          2066\n",
       "integration          384       896          3164\n",
       "structural           397       857          3379\n",
       "relative             427       802          2166\n",
       "economics            429       793          2819\n",
       "transaction          430       792          3366\n",
       "pension              444       771          3336\n",
       "adjustment           447       763          2764\n",
       "rapid                465       740          2682\n",
       "implement            489       697          2037\n",
       "institutional        496       691          2837\n",
       "recession            507       679          3166\n",
       "accord               529       643       1000000\n",
       "distribution         566       583          2115\n",
       "efficiency           574       570          2699\n",
       "subsidy              577       567          3738\n",
       "substantial          582       565          2267\n",
       "recovery             585       561          2181\n",
       "deposit              601       548          4135\n",
       "innovation           606       546          3230\n",
       "objective            611       539          2144\n",
       "corruption           630       524          3302\n",
       "mechanism            633       520          2379\n",
       "rapidly              634       520          2251\n",
       "implication          653       492          2171\n",
       "prospect             701       454          2071\n",
       "efficient            703       454          2654\n",
       "stable               709       446          2569\n",
       "sustain              717       442          2643\n",
       "expansion            721       437          2427\n",
       "...                  ...       ...           ...\n",
       "destined           17228         1       1000000\n",
       "noble              17241         1       1000000\n",
       "recuperate         17316         1       1000000\n",
       "obligated          17359         1       1000000\n",
       "users              17455         1       1000000\n",
       "auxiliary          17475         1       1000000\n",
       "stressed           17635         1       1000000\n",
       "consults           17677         1       1000000\n",
       "gratitude          17774         1       1000000\n",
       "encountered        17811         1       1000000\n",
       "nations            17919         1       1000000\n",
       "solitary           17933         1       1000000\n",
       "torture            17940         1       1000000\n",
       "packet             17978         1       1000000\n",
       "hydrogen           17987         1       1000000\n",
       "occupied           18008         1       1000000\n",
       "obtained           18040         1       1000000\n",
       "communities        18084         1       1000000\n",
       "deserted           18086         1       1000000\n",
       "islands            18089         1       1000000\n",
       "directs            18106         1       1000000\n",
       "interrupted        18108         1       1000000\n",
       "favour             18122         1       1000000\n",
       "converted          18160         1       1000000\n",
       "exceptions         18178         1       1000000\n",
       "salvation          18216         1       1000000\n",
       "academy            18219         1       1000000\n",
       "comprehension      18238         1       1000000\n",
       "ignorant           18280         1       1000000\n",
       "acquired           18282         1       1000000\n",
       "mama               18298         1       1000000\n",
       "homosexual         18305         1       1000000\n",
       "violently          18342         1       1000000\n",
       "ignored            18385         1       1000000\n",
       "proposes           18394         1       1000000\n",
       "innocence          18464         1       1000000\n",
       "assuming           18532         1       1000000\n",
       "obstacles          18612         1       1000000\n",
       "posture            18642         1       1000000\n",
       "princess           18696         1       1000000\n",
       "divinity           18708         1       1000000\n",
       "demon              18767         1       1000000\n",
       "caused             18783         1       1000000\n",
       "qualities          18830         1       1000000\n",
       "salute             18875         1       1000000\n",
       "restitution        18892         1       1000000\n",
       "scenery            18912         1       1000000\n",
       "invited            19074         1       1000000\n",
       "salaries           19114         1       1000000\n",
       "studied            19116         1       1000000\n",
       "\n",
       "[1182 rows x 3 columns]"
      ]
     },
     "execution_count": 1129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows=100\n",
    "economics_vocab_with_cognates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imf_rank</th>\n",
       "      <th>imf_freq</th>\n",
       "      <th>general_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>currency</th>\n",
       "      <td>136</td>\n",
       "      <td>2328</td>\n",
       "      <td>3297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poverty</th>\n",
       "      <td>146</td>\n",
       "      <td>2123</td>\n",
       "      <td>2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spending</th>\n",
       "      <td>175</td>\n",
       "      <td>1853</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inequality</th>\n",
       "      <td>206</td>\n",
       "      <td>1556</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>228</td>\n",
       "      <td>1432</td>\n",
       "      <td>3476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul</th>\n",
       "      <td>3146</td>\n",
       "      <td>41</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hurdle</th>\n",
       "      <td>3152</td>\n",
       "      <td>41</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isolation</th>\n",
       "      <td>3154</td>\n",
       "      <td>41</td>\n",
       "      <td>3753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>societal</th>\n",
       "      <td>3155</td>\n",
       "      <td>41</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liquidate</th>\n",
       "      <td>3156</td>\n",
       "      <td>41</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            imf_rank  imf_freq  general_rank\n",
       "currency         136      2328          3297\n",
       "poverty          146      2123          2080\n",
       "spending         175      1853          2082\n",
       "inequality       206      1556       1000000\n",
       "banking          228      1432          3476\n",
       "...              ...       ...           ...\n",
       "paul            3146        41       1000000\n",
       "hurdle          3152        41       1000000\n",
       "isolation       3154        41          3753\n",
       "societal        3155        41       1000000\n",
       "liquidate       3156        41       1000000\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 1130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows=10\n",
    "economics_vocab_no_cognates[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"economics_vocab.txt\",\"w\")\n",
    "f.write(\"\"\"#List of 2000 economics related words \n",
    "#Ranked by frequency of occurrence in the imf finance and development magazine\n",
    "#https://www.imf.org/external/pubs/ft/fandd/\n",
    "#Words with Spanish cognates removed\\n\"\"\")\n",
    "for i in economics_vocab_no_cognates[:2000].index:\n",
    "    f.write((i+\"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
